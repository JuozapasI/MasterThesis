\section{Data Acquisition}

\paragraph{\textbf{scRNAseq datasets}}

We analyzed datasets from four different tissues: brain, blood, lung, and eye.
All samples, except for two Peripheral Blood Mononuclear Cell (PBMC) datasets, were generated using the 10x Genomics v3.1 protocol.
The two exceptions were prepared using the inDrops2 and inDrops protocols.
All protocols capture short reads from the 3' ends of RNA molecules.
The datasets are publicly available, with sources and additional details provided in Table \ref{tab:datasets}.

\input{"textparts/tables/datasets.tex"}

\paragraph{\textbf{Genome and Transcriptomic references}}

The human genome GRCh38 was used in this project, downloaded from the \href{https://ftp.ensembl.org/pub/release-113/fasta/homo_sapiens/dna/}{Ensembl website}.

Four transcriptomic references were analyzed (see Table \ref{tab:references} for details).
To ensure compatibility with the genome FASTA file, chromosome name prefixes ('chr') were removed from references that included them,
as the genome file does not use these prefixes.
This modification was performed using basic Linux command-line tools.
Additionally, for the NCBI reference, chromosome names were converted to Ensembl-style notation using a custom Python script
(e.g., 1, 2, 3 instead of NC\_000001.11, NC\_000002.12, NC\_000003.12, etc.).

For references that do not have 'gene' entries, such entries were added
(entry that spans all the components of a particular gene).

\input{"textparts/tables/references.tex"}

\paragraph{\textbf{Gene Prediction Tracks and Conservation Scores}}

Gene predictions (tracks 'AUGUSTUS', 'Geneid genes', 'Gescan genes', 'SGP genes', 'SIB genes') and 
genome conservation scores ('phastCons100way' track) were downloaded from \href{https://genome.ucsc.edu/}{UCSC genome browser}.

\section{Computational Tools and Environment}

All analyses were performed on a high-performance computing (HPC) cluster running a Linux environment.
Software packages and tools were managed using Conda.
The main tools and their versions are listed in Table \ref{tab:tools}.
The exact Conda environment specifications (YAML file) can be found on [GitHub](GIVE LINK).
Besides tools managed by conda, also basic command line tools were used (e.g. \textit{awk, wc, grep, sed, uniq, sort) and similar).
The specific functions and parameters used are described in the following sections, where the general analysis pipeline will be explained.

\input{"textparts/tables/tools.tex"}

\section{Enhancing transcriptomic reference}

The full scripts are available on the GITHUBLINK, here only general description of the workflow and used tools provided.
Here is provided general description of the pipeline:

\begin{enumerate}
    \item Map reads with initial transcriptomic reference.
    \item Take unassigned (and uniquely mapped) reads.
    \item Split into intersecting and intergenic reads.
    \begin{enumerate}
        \item For intersecting:
        \begin{enumerate}
            \item Resolve overlapping genes that have unassigned reads.
            \item From the second reference and further: add genes to the original GTF that contain unassigned reads
            and do not overlap with entries from the original one.
        \end{enumerate}
        \item For intergenic:
        \begin{enumerate}
            \item Cluster.
            \item Filter-out relatively small clusters (custom threshold).
            \item For the first reference only: filter-out AT-rich reads.
            \item For reads that are left, repeat from the beginning with the next reference.
            \item For the last reference only: clusters that start just after 3' ends are assigned to genes (i.e., extend genes).
            \item For the last reference only: add largest intergenic unexplained regions to GTF (INTERGENIC entries).
        \end{enumerate}
    \end{enumerate}
    \item Create final GTF and map initial sequences to it.
    \item Create list of large (\textgreater 5CPM) intergenic clusters.
\end{enumerate}

\paragraph{Mapping and filtering bam files}
Reads were mapped using STARsolo (\cite{Kaminow2021}), both in the case of mapping from fastq and bam input files.
Parameters were used the same for all samples (see GITHUBLINK), except the ones regarding barcode geometries.
Filtering unassigned reads was done using \textit{awk}, taking those that have valid barcode
(i.e. filtering out those reads that have barcode length not equal to the defined length)
and were not assigned to any gene (i.e. had 'GN:Z:-' tag).
Also, only uniquelly mapped reads were taken (tag 'NH:i:1').

\paragraph{Classifying reads as 'intergenic' or 'intersecting' and reads clustering}
Intersections of unassigned reads with references were checked using \textit{bedtools intersect} command.
Those that intersect with genes were classified as 'intersecting', and those that do not â€“ as 'intergenic'.
Intersections were check in strand-specific manner (bedtools '-s' flag).
The intergenic reads were clustered using \textit{bedtools merge}, again in a strand-specific manner.

\paragraph{Manipulating transcriptomic references}
The overlapping gene resolving and construction of enhanced transcriptomic reference was done using custom R script,
particularly \textit{rtracklayer} library.
The criterions for the resolving of overlapping genes are following:
\begin{enumerate}
  \item \textbf{Gene type}: prefer protein coding genes over other types, lncRNA over remaining (e.g. pseudogenes).
  \item \textbf{Level}: some annotations have 'level' field, indicating if the the is verified (score 1), manually annotated (score 2)
  or automatically annotated (score 3). Lower 'level' score was preffered.
  \item \textbf{Intersection types}: if 5' end gene is overlapping with 3' end of gene, 5' end of gene was shortened,
  as data we were using is generated using 3' end method, suggesting that reads in the 5' end regions of genes were not expected.
\end{enumerate}

This is rough description of the usage of various tools in the pipeline given above, the pipeline itself was implemented using GNU MAKE.

\section{Intergenic regions}

\paragraph{Extracting intergenic regions}

The intergenic reads were extracted as described in the previous section (i.e. taked unassigned reads that do not overlap with any reference used)
and merged into clusters using \textit{bedtools merge} command.
Clusters were filtered based on cluster size, to include at least 5 reads per million (CPM)
(computed from the totol number of primary reads in the datasets).
The filtering was done using \textit{awk}.

Cluster locations were adjusted using deeptools (to compute coverage) and custom python script,
to avoid reads containing long introns making the intergenic clusters very wide.
To accomplish this, for each intergenic region the maximum coverage location was found and extended to include neighbouring regions that had
at least $max\_value / 2$ coverage.
In such way, intergenic regions were reduced to include 'peaks' of reads.

The lists acquired from all samples were then merged using \textit{bedtools merge} function.
This combined filter then was filtered to contain only regions detected in sufficient number of samples
(i.e. in all 'eye', 'lung', 'brain', 'PBMC\_10x' or 'PBMC\_indrops' samples).
Additionally, for each entry it was determined whether it overlaps with predicted genes from UCSC gene prediction archive
(using \textit{bedtools merge}) and distances to the closest GENCODE or RefSec genes were found (\textit{bedtools closest}).

This filtered combined list were then converted into GTF format and unassigned reads from each sample were mapped using this
combined intergenic annotation.

\paragraph{Analysis of cell-gene matrices}

The matrices produced by STAR were filtered, allowing cells that have sufficient number of reads (thresholds selected manually),
and only genes that were expressed in at least 3 cells.
Also cells were filtered based on mitochondrial gene count (allowing up to 10\% of mitochondrial gene in a cell).
Doublets were filtered using \textit{scrublet}.
Afterwards, matrices were normalized (using \textit{normalize\_total} function from \textit{scanpy} package) and log-transformaed
(i.e. for each entry $x = log(x+1)$, \textit{log1p} function from \textit{scanpy}).

Then only highly-variable genes (min\_mean=0.0125, max\_mean=3, min\_disp=0.5) were selected.
For the clustering, principal components (PCs) were computed and optimal number of them were selected based on elbow rule (manually).
For blood samples, cells were annotated automatically using CellTypist
(for other samples, annotation was skipped, as it was not in the main focus of this project).
Then visualizations were made using UMAP embeddings (functions from the same \textit{scanpy} package).

All the scripts with descriptions can be found in the GITHUBLINK.
